{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPcGWx9xzvH49rwEOyYzii+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuMinHo/GAI_project/blob/main/gui_gai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from vllm import LLM, SamplingParams\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pymupdf as fitz\n",
        "import pymupdf4llm\n",
        "import hashlib\n",
        "import logging\n",
        "\n",
        "# LLM 초기화\n",
        "llm = LLM(model=\"llava-hf/llava-v1.6-mistral-7b-hf\", dtype='half', max_model_len=8192)\n",
        "sampling_params = SamplingParams(temperature=0.7, max_tokens=512)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# RAGPipeline 클래스\n",
        "class RAGPipeline:\n",
        "    def __init__(self):\n",
        "        self.llm = llm\n",
        "        self.sampling_params = sampling_params\n",
        "        self.embedder = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "        self.index = faiss.IndexFlatL2(self.embedder.get_sentence_embedding_dimension())\n",
        "        self.chunks = []\n",
        "        self.processed_files = {}\n",
        "\n",
        "    def get_file_hash(self, file_path: str) -> str:\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            return hashlib.md5(f.read()).hexdigest()\n",
        "\n",
        "    def indexing_pdf(self, pdf_path: str):\n",
        "        file_hash = self.get_file_hash(pdf_path)\n",
        "        if file_hash in self.processed_files:\n",
        "            logging.info(f\"{pdf_path} has already been processed.\")\n",
        "            return\n",
        "        self.processed_files[file_hash] = pdf_path\n",
        "\n",
        "        doc = fitz.open(pdf_path)\n",
        "        markdown_text = pymupdf4llm.to_markdown(doc)\n",
        "        doc.close()\n",
        "        chunks = markdown_text.split('\\n')\n",
        "        self.chunks.extend(chunks)\n",
        "        embeddings = self.embedder.encode(chunks)\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "    def process_query(self, query: str, top_k: int = 5):\n",
        "        query_embedding = self.embedder.encode([query])\n",
        "        distances, indices = self.index.search(query_embedding, top_k)\n",
        "        return [self.chunks[i] for i in indices[0]]\n",
        "\n",
        "    def prompt_template(self, query: str, context: list):\n",
        "        prompt = f\"\"\"\n",
        "        [INST]\n",
        "        Answer the question based on the following context:\n",
        "        {context}\n",
        "\n",
        "        Question: {query}\n",
        "        [/INST]\n",
        "        \"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def generate_response(self, query: str, context: list):\n",
        "        prompt = self.prompt_template(query, context)\n",
        "        output = self.llm.generate([prompt], self.sampling_params)\n",
        "        return output[0].outputs[0].text\n",
        "\n",
        "    def answer_query(self, query: str, top_k: int = 5):\n",
        "        context = self.process_query(query, top_k)\n",
        "        return self.generate_response(query, context)\n",
        "\n",
        "\n",
        "# LLaVAImageQAProcessor 클래스\n",
        "class LLaVAImageQAProcessor:\n",
        "    def __init__(self):\n",
        "        self.llm = llm\n",
        "        self.sampling_params = sampling_params\n",
        "\n",
        "    def process_image(self, image_path, question):\n",
        "        prompt = f\"\"\"[INST] <image>\n",
        "        Explain this image in detail:\n",
        "\n",
        "        Question: {question}\n",
        "        [/INST]\"\"\"\n",
        "\n",
        "        image = Image.open(image_path)\n",
        "        inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"image\": image}}\n",
        "        outputs = self.llm.generate(inputs, sampling_params=self.sampling_params)\n",
        "        return outputs[0].outputs[0].text\n",
        "\n",
        "\n",
        "# UI 통합\n",
        "def create_ui():\n",
        "    pipeline = RAGPipeline()\n",
        "    image_processor = LLaVAImageQAProcessor()\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        with gr.Row():\n",
        "            chat_mode = gr.Radio(\n",
        "                choices=[\"General Chat\", \"RAG Chat\"],\n",
        "                value=\"General Chat\",\n",
        "                label=\"Mode\"\n",
        "            )\n",
        "\n",
        "        with gr.Column():\n",
        "            chatbot = gr.Chatbot()\n",
        "\n",
        "            with gr.Row():\n",
        "                file_upload = gr.File(label=\"Upload File\")\n",
        "                msg_input = gr.Textbox(placeholder=\"Type your message here...\")\n",
        "                send_btn = gr.Button(\"Send\")\n",
        "\n",
        "        def process_input(chat_mode, file_upload, msg_input):\n",
        "            if chat_mode == \"General Chat\" and file_upload and file_upload.name.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                response = image_processor.process_image(file_upload.name, msg_input)\n",
        "                return chatbot.update([(msg_input, response)])\n",
        "            elif chat_mode == \"RAG Chat\" and file_upload and file_upload.name.endswith('.pdf'):\n",
        "                pipeline.indexing_pdf(file_upload.name)\n",
        "                response = pipeline.answer_query(msg_input)\n",
        "                return chatbot.update([(msg_input, response)])\n",
        "            else:\n",
        "                return chatbot.update([(msg_input, \"Unsupported input or mode.\")])\n",
        "\n",
        "        send_btn.click(\n",
        "            process_input,\n",
        "            inputs=[chat_mode, file_upload, msg_input],\n",
        "            outputs=chatbot\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_ui()\n",
        "    demo.launch()\n"
      ],
      "metadata": {
        "id": "6ExNxvLCviSN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}