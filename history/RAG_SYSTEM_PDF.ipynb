{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install vllm transformers triton PyPDF2 Pillow sentence_transformers numpy typing faiss-gpu semchunk gradio docling pymupdf4llm fitz frontend tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from PyPDF2 import PdfReader\n",
    "import semchunk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "#from docling.document_converter import DocumentConverter\n",
    "#import fitz\n",
    "import pymupdf4llm  # PyMuPDF4LLM\n",
    "import pymupdf as fitz\n",
    "import os\n",
    "\n",
    "if not os.path.exists('static'):\n",
    "    os.makedirs('static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 및 토크나이저 초기화\n",
    "llm = LLM(model=\"llava-hf/llava-v1.6-mistral-7b-hf\", dtype='half', max_model_len=8192)\n",
    "embedder = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# FAISS 인덱스 초기화\n",
    "dim = embedder.get_sentence_embedding_dimension()\n",
    "print(dim)\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "\"\"\"\n",
    "def process_pdf(file_path: str, query: str = None) -> List[str]:\n",
    "    # PDF를 Markdown으로 변환\n",
    "    converter = DocumentConverter()\n",
    "    result = converter.convert(file_path)\n",
    "    markdown_text = result.document.export_to_markdown()\n",
    "\n",
    "    # 청킹\n",
    "    chunk_size = 200\n",
    "    chunker = semchunk.chunkerify('umarbutler/emubert', chunk_size)\n",
    "    chunks = chunker(markdown_text)\n",
    "    print(chunks)\n",
    "\n",
    "    # 임베딩 및 인덱싱\n",
    "    chunks_embeddings = embedder.encode(chunks)\n",
    "    index.add(chunks_embeddings)\n",
    "\n",
    "    if query:\n",
    "        # 쿼리 처리\n",
    "        query_embedding = embedder.encode([query])\n",
    "        top_k = 5\n",
    "        distances, indices = index.search(query_embedding, top_k)\n",
    "        results = [chunks[idx] for idx in indices[0] if idx < len(chunks)]\n",
    "        return results\n",
    "\n",
    "    return chunks\n",
    "\"\"\"\n",
    "def process_pdf(file_path: str, query: str = None) -> List[str]:\n",
    "    # PDF를 Markdown으로 변환\n",
    "    doc = fitz.open(file_path)\n",
    "    markdown_text = pymupdf4llm.to_markdown(doc)\n",
    "    doc.close()\n",
    "\n",
    "    # 청킹\n",
    "    chunk_size = 200\n",
    "    chunker = semchunk.chunkerify('umarbutler/emubert', chunk_size)\n",
    "    chunks = chunker(markdown_text)\n",
    "    print(chunks)\n",
    "\n",
    "    # 임베딩 및 인덱싱\n",
    "    chunks_embeddings = embedder.encode(chunks, batch_size=32)\n",
    "    index.add(chunks_embeddings)\n",
    "\n",
    "    if query:\n",
    "        # 쿼리 처리\n",
    "        query_embedding = embedder.encode([query])\n",
    "        top_k = 5\n",
    "        distances, indices = index.search(query_embedding, top_k)\n",
    "        results = [chunks[idx] for idx in indices[0] if idx < len(chunks)]\n",
    "        return results\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 대화 히스토리 저장\n",
    "conversation_history = []\n",
    "\n",
    "def generate_answer(question: str, context: str = \"\") -> str:\n",
    "    global conversation_history\n",
    "\n",
    "    if context:\n",
    "        prompt = f\"\"\"[INST] You are an AI assistant specialized in analyzing documents. Your task is to answer the following question based solely on the provided context. Follow these guidelines:\n",
    "\n",
    "1. Carefully analyze the given context.\n",
    "2. If the context contains relevant information, provide a clear and concise answer.\n",
    "3. If the context lacks relevant information, explicitly state: \"The provided context does not contain information to answer this question.\"\n",
    "4. Do not use any external knowledge or make assumptions beyond the given context.\n",
    "5. If the question requires clarification, ask for more specific details.\n",
    "\n",
    "Previous conversation:\n",
    "{' '.join(conversation_history)}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: [/INST]\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"[INST] You are an AI assistant with expertise in various fields. Please follow these steps to respond:\n",
    "\n",
    "1. Analyze the user's question carefully.\n",
    "2. Identify the main topic and any subtopics in the question.\n",
    "3. Provide a clear, concise, and informative answer.\n",
    "4. If the question is ambiguous, ask for clarification before answering.\n",
    "5. If you're unsure about any part of the answer, explicitly state your uncertainty.\n",
    "\n",
    "Previous conversation:\n",
    "{' '.join(conversation_history)}\n",
    "\n",
    "User's question: {question}\n",
    "\n",
    "Your response: [/INST]\"\"\"\n",
    "\n",
    "    inputs = {\"prompt\": prompt}\n",
    "    sampling_params = SamplingParams(temperature=0.2, max_tokens=256)\n",
    "    outputs = llm.generate(inputs, sampling_params=sampling_params)\n",
    "    return outputs[0].outputs[0].text\n",
    "\n",
    "def add_message(history, message):\n",
    "    global conversation_history\n",
    "\n",
    "    for file in message[\"files\"]:\n",
    "        try:\n",
    "            history.append({\"role\": \"user\", \"content\": {\"path\": file}})\n",
    "            text = message.get(\"text\", None)\n",
    "            if text:\n",
    "                history.append({\"role\": \"user\", \"content\": text})\n",
    "                conversation_history.append(f\"User: {text}\")\n",
    "\n",
    "            chunks = process_pdf(file, text)\n",
    "            print(chunks)\n",
    "            #context = \" \".join(chunks) if chunks else \"\"\n",
    "            #result = generate_answer(text, context)\n",
    "            #history.append({\"role\": \"assistant\", \"content\": result})\n",
    "            #conversation_history.append(f\"Assistant: {result}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            history.append({\"role\": \"assistant\", \"content\": f\"Invalid PDF file: {str(e)}\"})\n",
    "\n",
    "    if not message[\"files\"] and message[\"text\"]:\n",
    "        history.append({\"role\": \"user\", \"content\": message[\"text\"]})\n",
    "        conversation_history.append(f\"User: {message['text']}\")\n",
    "        #result = generate_answer(message[\"text\"])\n",
    "        #history.append({\"role\": \"assistant\", \"content\": result})\n",
    "        #conversation_history.append(f\"Assistant: {result}\")\n",
    "\n",
    "    return history, gr.MultimodalTextbox(value=None, interactive=False)\n",
    "\n",
    "def bot(history: list):\n",
    "    response = history[-1][\"content\"]\n",
    "    rsps = generate_answer(response)\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    for character in rsps:\n",
    "        history[-1][\"content\"] += character\n",
    "        time.sleep(0.05)\n",
    "        yield history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Gradio 인터페이스 설정\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\", bubble_full_width=False, type=\"messages\")\n",
    "    chat_input = gr.MultimodalTextbox(\n",
    "        interactive=True,\n",
    "        file_count=\"multiple\",\n",
    "        placeholder=\"Enter message or upload file...\",\n",
    "        show_label=False,\n",
    "    )\n",
    "    chat_msg = chat_input.submit(\n",
    "        add_message, [chatbot, chat_input], [chatbot, chat_input]\n",
    "    )\n",
    "    bot_msg = chat_msg.then(bot, chatbot, chatbot, api_name=\"bot_response\")\n",
    "    bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, [chat_input])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
