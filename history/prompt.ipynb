{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNJ6s03/CTc/BNWSXPRjA9e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuMinHo/GAI_project/blob/main/prompt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf pymupdf4llm gradio faiss-gpu sentence-transformers semchunk vllm triton"
      ],
      "metadata": {
        "id": "f6lcZv4pjCbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import faiss\n",
        "import numpy as np\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "import time\n",
        "import semchunk\n",
        "import pymupdf as fitz\n",
        "import pymupdf4llm\n",
        "from vllm import LLM, SamplingParams\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from PIL import Image\n",
        "import hashlib\n",
        "import logging"
      ],
      "metadata": {
        "id": "K3lwUmVwjBTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(model=\"llava-hf/llava-v1.6-mistral-7b-hf\", dtype='half', max_model_len=8192)\n",
        "sampling_params = SamplingParams(temperature=0.7, max_tokens=512)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\"\"\"\n",
        "PDF 파일 RAG를 위한 Pipeline class\n",
        "\"\"\"\n",
        "class RAGPipeline:\n",
        "    def __init__(self):\n",
        "        # lava-hf/llava-v1.6-mistral-7b-hf를 사용\n",
        "        self.llm = llm\n",
        "\n",
        "        # Sampling parameters 설정\n",
        "        self.sampling_params = sampling_params\n",
        "\n",
        "        # embedding\n",
        "        self.embedder = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "        self.chunker = semchunk.chunkerify('gpt-4', 200)\n",
        "        self.index = faiss.IndexFlatL2(self.embedder.get_sentence_embedding_dimension())\n",
        "        self.chunks = []\n",
        "        self.processed_files = {} # {file_hash: file_path}\n",
        "\n",
        "    # Gradio는 file을 업로드할 때 임시 경로를 사용하므로, 파일의 hash를 통해 중복을 확인하기 위함\n",
        "    def get_file_hash(self, file_path: str) -> str:\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            return hashlib.md5(f.read()).hexdigest()\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    PDF indexing\n",
        "    - PDF를 입력받아, pymupdf4llm을 통해 Markdown으로 변환\n",
        "    - Markdown을 semchunk를 통해 Chunk로 나눔\n",
        "    - Chunk를 SentenceTransformer를 통해 embedding\n",
        "    - embedding 결과를 faiss index에 추가\n",
        "    \"\"\"\n",
        "\n",
        "    def indexing_pdf(self, pdf_path: List[str]):\n",
        "\n",
        "        # 이미 indexing을 진행했던 파일인지 확인\n",
        "        for pdf in pdf_path:\n",
        "            try:\n",
        "                file_hash = self.get_file_hash(pdf)\n",
        "                if file_hash in self.processed_files:\n",
        "                    logging.info(f\"{pdf} has already been processed before\")\n",
        "                    continue\n",
        "\n",
        "                # indexing한 적이 없는 파일인 경우, {file_hash : pdf경로} 를 self.processed_files에 추가\n",
        "                self.processed_files[file_hash] = pdf\n",
        "                logging.info(f\"Processing new file: {pdf}\")\n",
        "\n",
        "                # PDF를 로드하여 Markdown으로 변환\n",
        "                doc = fitz.open(pdf)\n",
        "                markdown_text = pymupdf4llm.to_markdown(doc)\n",
        "                doc.close()\n",
        "\n",
        "                # 변환된 markdown_text를 Chunk로 나누기\n",
        "                chunks = self.chunker(markdown_text)\n",
        "\n",
        "                # chunks를 list에 추가\n",
        "                self.chunks.extend(chunks)\n",
        "\n",
        "                # chunks를 embedding\n",
        "                chunks_embeddings = self.embedder.encode(chunks)\n",
        "\n",
        "                # chunks를 embedding한 결과를 faiss index에 추가\n",
        "                self.index.add(chunks_embeddings)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error in indexing {pdf_path}: {e}\")\n",
        "\n",
        "        logging.info(f\"Processed {len(pdf_path)} files. Total unique files: {len(self.processed_files)}\")\n",
        "\n",
        "    \"\"\"\n",
        "    query 처리 및 검색\n",
        "    - query를 SentenceTransformer를 통해 embedding\n",
        "    - embedding 결과를 faiss index를 통해 top_k개 검색\n",
        "    - 검색 결과를 반환\n",
        "    \"\"\"\n",
        "    def process_query(self, query: str, top_k: int = 5) -> List[str]:\n",
        "        # 쿼리 임베딩 및 관련 컨텍스트 검색\n",
        "        query_embedding = self.embedder.encode([query])\n",
        "        distances, indices = self.index.search(query_embedding, top_k)\n",
        "        return [self.chunks[i] for i in indices[0]]\n",
        "\n",
        "    #쿼리를 키워드에 따라 분류\n",
        "def classify_query(query):\n",
        "    query = query.lower()\n",
        "    categories_keywords = {\n",
        "        \"Summary Request\": [\"summary\", \"summarize\", \"overview\", \"key points\", \"main content\", \"gist\"],\n",
        "        \"Information Retrieval\": [\"definition\", \"meaning\", \"explain\", \"reason\", \"clarify\", \"purpose\"],\n",
        "        \"Calculation/Analysis Request\": [\"analysis\", \"calculate\", \"steps\", \"process\", \"formula\", \"problem\"],\n",
        "        \"Comparison Request\": [\"compare\", \"difference\", \"similarity\", \"better\", \"features\", \"versus\"],\n",
        "        \"Task Instruction\": [\"write\", \"code\", \"example\", \"generate\", \"task\", \"perform\", \"how to\", \"steps to\", \"create\"]\n",
        "    }\n",
        "    for category, keywords in categories_keywords.items():\n",
        "        if any(keyword in query for keyword in keywords):\n",
        "            return category\n",
        "    return \"Uncategorized\"\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Prompt template 함수\n",
        "    - query의 유형을 나누고 각 유형마다 다른 intruction을 추가\n",
        "    - query와 context를 입력받아, LLM에 입력할 prompt를 생성\n",
        "    - prompt에는 system message, context, query, instruction이 포함\n",
        "    - 생성된 prompt를 반환\n",
        "    \"\"\"\n",
        "    def prompt_template(self, query: str, context: List[str]) -> str:\n",
        "        query_type = self.classfiy_query(query)\n",
        "\n",
        "        if query_type == \"Summary Request\":\n",
        "          instructions = (\n",
        "            \"- Summarize the key concepts of the context in a concise and clear manner.\\n\"\n",
        "            \"- Avoid repeating the same information.\\n\"\n",
        "            \"- Summarize each paragraph into one sentence to maintain clarity and focus on the main points.\\n\"\n",
        "            \"- Provide a step-by-step summary of the process or procedure described in the context, if applicable.\\n\"\n",
        "            \"- You are an AI personal assistant. Respond in a clear and professional manner.\"\n",
        "        )\n",
        "        elif query_type == \"Information Retrieval\":\n",
        "          instructions = (\n",
        "            \"- Provide a clear and concise answer to the user's question based on the given context.\\n\"\n",
        "            \"- Focus on accuracy and omit irrelevant information to provide a precise answer.\\n\"\n",
        "            \"- If the context lacks sufficient information, respond with 'The information is insufficient.'\\n\"\n",
        "            \"- Do not fabricate or add information that is not present in the given context.\\n\"\n",
        "            \"- For example, if the context states 'June 5th' as the date, your response should also use 'June 5th' rather than rephrasing it as 'June 5th, 2001' or '5th of June.'\\n\"\n",
        "            \"- You are an AI personal assistant. Respond in a clear and professional manner.\"\n",
        "        )\n",
        "        elif query_type == \"Calculation/Analysis Request\":\n",
        "          instructions = (\n",
        "            \"- Write the calculation or analysis process step-by-step in the format 'step 1:', 'step 2:', etc.\\n\"\n",
        "            \"- Ensure that your explanation is logical and easy to understand.\\n\"\n",
        "            \"- If the context lacks information required for the calculation or analysis, clearly identify which part of the information is missing and explain why it is needed.\\n\"\n",
        "            \"- Clearly state the final result of the calculation.\\n\"\n",
        "            \"- Accurately indicate the units (e.g., $, %, km) of the results.\\n\"\n",
        "            \"- Assume you are a teacher explaining the calculation or analysis to a student, and provide a clear and helpful explanation.\"\n",
        "        )\n",
        "        elif query_type == \"Comparison Request\":\n",
        "          instructions = (\n",
        "            \"- Explain the similarities and differences between the items or concepts in a logical and structured way.\\n\"\n",
        "            \"- If necessary, provide advantages and disadvantages for each item in a table or bullet-point format.\\n\"\n",
        "            \"- Maintain fairness and objectivity, avoiding any biased statements.\\n\"\n",
        "            \"- Clearly state the criteria for comparison (e.g., speed, cost, efficiency) to ensure clarity.\\n\"\n",
        "            \"- Use concise and clear language to make the comparison easy to understand.\\n\"\n",
        "            \"- Assume you are a teacher explaining the comparison to a student, and provide a clear and insightful explanation.\"\n",
        "        )\n",
        "        elif query_type == \"Task Instruction\":\n",
        "          instructions = (\n",
        "            \"- Explain the steps required to perform the requested task in a clear and sequential manner.\\n\"\n",
        "            \"- Include code snippets, examples, or commands where necessary to make the instructions actionable.\\n\"\n",
        "            \"- Ensure the explanation is logical and easy to follow, avoiding unnecessary complexity.\\n\"\n",
        "            \"- If the context lacks sufficient information, specify what additional information is required to proceed.\\n\"\n",
        "            \"- Present the response as a professional AI assistant, ensuring the instructions are clean and precise.\\n\"\n",
        "            \"- Clearly describe the expected outcome or result of the task.\\n\"\n",
        "            \"- Provide additional tips or warnings if applicable.\"\n",
        "        )\n",
        "\n",
        "        system_message = \"\"\"You are an AI assistant tasked with answering questions based on provided context. Your role is to:\n",
        "                            1. Carefully analyze the given context\n",
        "                            2. Provide accurate and relevant information\n",
        "                            3. Synthesize a coherent response\n",
        "                            4. Maintain objectivity and clarity\n",
        "                            If the context doesn't contain sufficient information, state so clearly.\"\"\"\n",
        "\n",
        "        context_str = \"\\n\".join([f\"Context {i+1}: {ctx}\" for i, ctx in enumerate(context)])\n",
        "\n",
        "        prompt = f\"\"\"[INST] {system_message}\n",
        "\n",
        "            Relevant information:\n",
        "            {context_str}\n",
        "\n",
        "            User's Quetion: {query}\n",
        "\n",
        "            Instructions:\n",
        "            - Answer the query using only the information provided in the context.\n",
        "            - If the context doesn't contain enough information to fully answer the query, acknowledge this limitation in your response.\n",
        "            - Provide a concise yet comprehensive answer.\n",
        "            - Do not introduce information not present in the given context.\n",
        "            - Privide in complete sentences in English always.\n",
        "            - Check once again your response so that the user can be provided precise information.\n",
        "            {instructions}\n",
        "\n",
        "            Please provide your response below:\n",
        "            [/INST]\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    \"\"\"\n",
        "    query와 context를 입력받아, LLM을 통해 답변 생성\n",
        "    \"\"\"\n",
        "    def generate_response(self, query: str, context: List[str]) -> str:\n",
        "        prompt = self.prompt_template(query, context)\n",
        "        output = self.llm.generate([prompt], self.sampling_params)\n",
        "        return output[0].outputs[0].text\n",
        "\n",
        "    def answer_query(self, query: str, top_k: int = 5) -> str:\n",
        "        retrieved_contexts = self.process_query(query, top_k)\n",
        "        return self.generate_response(query, retrieved_contexts)\n",
        "\n",
        "class LLaVAImageQAProcessor:\n",
        "    def __init__(self):\n",
        "        self.llm = llm\n",
        "        self.sampling_params = sampling_params\n",
        "\n",
        "    def get_prompt(self, question: str):\n",
        "        return f\"\"\"[INST]\n",
        "                    Explain me about this image precisely in bullet points.\n",
        "                    Your response shuold be in complete sentences.\n",
        "                    <image>\\n{question} [/INST]\"\"\"\n",
        "\n",
        "    def process_image(self, image_path, question):\n",
        "        prompt = self.get_prompt(question)\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        inputs = {\n",
        "            \"prompt\": prompt,\n",
        "            \"multi_modal_data\": {\"image\": image},\n",
        "        }\n",
        "\n",
        "        outputs = self.llm.generate(inputs, sampling_params=self.sampling_params)\n",
        "        return outputs[0].outputs[0].text\n"
      ],
      "metadata": {
        "id": "Ri6XXp9LfqPI"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}