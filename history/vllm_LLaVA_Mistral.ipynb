{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이 파일은 vllm 을 통해 llava-hf/llava-v1.6-mistral-7b-hf 를 실행하기 위한 코드입니다.\n",
    "### Google Colab에서 vllm을 구동하기 위해 설치가 필요한 모듈들입니다. \n",
    "\n",
    "!pip install vllm transformers triton PyMuPDF Pillow sentence_transformers numpy typing faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "관련 Modules import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import fitz\n",
    "import spacy\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 및 토크나이저 초기화\n",
    "llm = LLM(model=\"llava-hf/llava-v1.6-mistral-7b-hf\", dtype='half', max_model_len=8192)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 200) -> List[str]:\n",
    "    doc = nlp(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    for sent in doc.sents:\n",
    "        if current_size + len(sent) > chunk_size and current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "        current_chunk.append(sent.text)\n",
    "        current_size += len(sent)\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def process_pdf(file_path: str) -> List[str]:\n",
    "    with fitz.open(file_path) as doc:\n",
    "        text = \" \".join([page.get_text() for page in doc])\n",
    "    return chunk_text(text)\n",
    "\n",
    "def index_chunks(chunks: List[str]):\n",
    "    global index\n",
    "    embeddings = embedder.encode(chunks)\n",
    "    embeddings_array = np.array(embeddings, dtype=np.float32)\n",
    "    index.add(embeddings_array)\n",
    "    return embeddings_array\n",
    "\n",
    "def retrieve_relevant_chunks(query: str, k: int = 5) -> List[Tuple[int, float]]:\n",
    "    query_vector = embedder.encode([query])\n",
    "    query_vector = np.array(query_vector, dtype=np.float32)  # 명시적으로 float32로 변환\n",
    "    D, I = index.search(query_vector, k)\n",
    "    return list(zip(I[0], D[0]))\n",
    "\n",
    "def generate_answer(question: str, context: str = \"\") -> str:\n",
    "    if context:\n",
    "        prompt = f\"\"\"[INST] You are a helpful AI assistant. Answer the following question based on the given context. If the context doesn't contain relevant information, say so. Do not make up information.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: [/INST]\"\"\"\n",
    "    else:\n",
    "        prompt = f\"[INST] You are a helpful AI assistant. {question} [/INST]\"\n",
    "\n",
    "    inputs = {\"prompt\": prompt}\n",
    "    sampling_params = SamplingParams(temperature=0.2, max_tokens=256)\n",
    "    outputs = llm.generate(inputs, sampling_params=sampling_params)\n",
    "    return outputs[0].outputs[0].text\n",
    "\n",
    "# PDF 처리 및 인덱싱\n",
    "pdf_chunks = process_pdf(\"/content/pdf_file.pdf\")\n",
    "\n",
    "# for debugging\n",
    "print(f\"Number of chunks: {len(pdf_chunks)}\")\n",
    "print(f\"First chunk: {pdf_chunks[0][:100]}...\")  # 첫 100자만 출력\n",
    "# end debugging\n",
    "\n",
    "chunk_embeddings = index_chunks(pdf_chunks)\n",
    "\n",
    "# for debugging\n",
    "print(f\"Number of embeddings: {len(chunk_embeddings)}\")\n",
    "# end debugging\n",
    "\n",
    "# 대화 루프\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    if user_input.startswith(\"PDF:\"):\n",
    "        # PDF 관련 질문 처리\n",
    "        query = user_input[4:].strip()\n",
    "        relevant_chunks = retrieve_relevant_chunks(query)\n",
    "        #for debugging\n",
    "        print(f\"Number of relevant chunks: {len(relevant_chunks)}\")\n",
    "        # end debugging\n",
    "        context = \" \".join([pdf_chunks[i] for i, _ in relevant_chunks if i < len(pdf_chunks)])\n",
    "        # for debugging\n",
    "        print(f\"Context length: {len(context)}\")\n",
    "        # end debugging\n",
    "        answer = generate_answer(query, context)\n",
    "    else:\n",
    "        # 일반 대화 처리\n",
    "        answer = generate_answer(user_input)\n",
    "    \n",
    "    print(\"AI:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
