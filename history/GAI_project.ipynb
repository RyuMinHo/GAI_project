{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import faiss\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import time\n",
    "import semchunk\n",
    "import pymupdf as fitz\n",
    "import pymupdf4llm\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from PIL import Image\n",
    "import hashlib\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=\"llava-hf/llava-v1.6-mistral-7b-hf\", dtype='half', max_model_len=8192)\n",
    "sampling_params = SamplingParams(temperature=0.7, max_tokens=512)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\"\"\"\n",
    "PDF 파일 RAG를 위한 Pipeline class\n",
    "\"\"\"\n",
    "class RAGPipeline:\n",
    "    def __init__(self):\n",
    "        # lava-hf/llava-v1.6-mistral-7b-hf를 사용\n",
    "        self.llm = llm\n",
    "\n",
    "        # Sampling parameters 설정\n",
    "        self.sampling_params = sampling_params\n",
    "\n",
    "        # embedding\n",
    "        self.embedder = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "        self.chunker = semchunk.chunkerify('gpt-4', 200)\n",
    "        self.index = faiss.IndexFlatL2(self.embedder.get_sentence_embedding_dimension())\n",
    "        self.chunks = []\n",
    "        self.processed_files = {} # {file_hash: file_path}\n",
    "\n",
    "    # Gradio는 file을 업로드할 때 임시 경로를 사용하므로, 파일의 hash를 통해 중복을 확인하기 위함\n",
    "    def get_file_hash(self, file_path: str) -> str:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            return hashlib.md5(f.read()).hexdigest()\n",
    "    \n",
    "    \"\"\"\n",
    "    PDF indexing\n",
    "    - PDF를 입력받아, pymupdf4llm을 통해 Markdown으로 변환\n",
    "    - Markdown을 semchunk를 통해 Chunk로 나눔\n",
    "    - Chunk를 SentenceTransformer를 통해 embedding\n",
    "    - embedding 결과를 faiss index에 추가\n",
    "    \"\"\"\n",
    "    \n",
    "    def indexing_pdf(self, pdf_path: List[str]):\n",
    "\n",
    "        # 이미 indexing을 진행했던 파일인지 확인\n",
    "        for pdf in pdf_path:\n",
    "            try:\n",
    "                file_hash = self.get_file_hash(pdf)\n",
    "                if file_hash in self.processed_files:\n",
    "                    logging.info(f\"{pdf} has already been processed before\")\n",
    "                    continue\n",
    "                \n",
    "                # indexing한 적이 없는 파일인 경우, {file_hash : pdf경로} 를 self.processed_files에 추가\n",
    "                self.processed_files[file_hash] = pdf\n",
    "                logging.info(f\"Processing new file: {pdf}\")\n",
    "\n",
    "                # PDF를 로드하여 Markdown으로 변환\n",
    "                doc = fitz.open(pdf)\n",
    "                markdown_text = pymupdf4llm.to_markdown(doc)\n",
    "                doc.close()\n",
    "                \n",
    "                # 변환된 markdown_text를 Chunk로 나누기\n",
    "                chunks = self.chunker(markdown_text)\n",
    "\n",
    "                # chunks를 list에 추가\n",
    "                self.chunks.extend(chunks)\n",
    "\n",
    "                # chunks를 embedding\n",
    "                chunks_embeddings = self.embedder.encode(chunks)\n",
    "\n",
    "                # chunks를 embedding한 결과를 faiss index에 추가\n",
    "                self.index.add(chunks_embeddings)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in indexing {pdf_path}: {e}\")\n",
    "        \n",
    "        logging.info(f\"Processed {len(pdf_path)} files. Total unique files: {len(self.processed_files)}\")\n",
    "\n",
    "    \"\"\"\n",
    "    query 처리 및 검색\n",
    "    - query를 SentenceTransformer를 통해 embedding\n",
    "    - embedding 결과를 faiss index를 통해 top_k개 검색\n",
    "    - 검색 결과를 반환\n",
    "    \"\"\"\n",
    "    def process_query(self, query: str, top_k: int = 5) -> List[str]:\n",
    "        # 쿼리 임베딩 및 관련 컨텍스트 검색\n",
    "        query_embedding = self.embedder.encode([query])\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        return [self.chunks[i] for i in indices[0]]\n",
    "    \n",
    "    \"\"\"\n",
    "    Prompt template 함수\n",
    "    - query와 context를 입력받아, LLM에 입력할 prompt를 생성\n",
    "    - prompt에는 system message, context, query, instruction이 포함\n",
    "    - 생성된 prompt를 반환\n",
    "    \"\"\"\n",
    "    def prompt_template(self, query: str, context: List[str]) -> str:\n",
    "        system_message = \"\"\"You are an AI assistant tasked with answering questions based on provided context. Your role is to:\n",
    "                            1. Carefully analyze the given context\n",
    "                            2. Provide accurate and relevant information\n",
    "                            3. Synthesize a coherent response\n",
    "                            4. Maintain objectivity and clarity\n",
    "                            If the context doesn't contain sufficient information, state so clearly.\"\"\"\n",
    "\n",
    "        context_str = \"\\n\".join([f\"Context {i+1}: {ctx}\" for i, ctx in enumerate(context)])\n",
    "        \n",
    "        prompt = f\"\"\"[INST] {system_message}\n",
    "\n",
    "            Relevant information:\n",
    "            {context_str}\n",
    "\n",
    "            User's Quetion: {query}\n",
    "\n",
    "            Instructions:\n",
    "            - Answer the query using only the information provided in the context.\n",
    "            - If the context doesn't contain enough information to fully answer the query, acknowledge this limitation in your response.\n",
    "            - Provide a concise yet comprehensive answer.\n",
    "            - Do not introduce information not present in the given context.\n",
    "            - Privide in complete sentences in English always.\n",
    "            - Check once again your response so that the user can be provided precise information.\n",
    "\n",
    "            Please provide your response below:\n",
    "            [/INST]\"\"\"\n",
    "            \n",
    "        return prompt\n",
    "    \n",
    "    \"\"\"\n",
    "    query와 context를 입력받아, LLM을 통해 답변 생성\n",
    "    \"\"\"\n",
    "    def generate_response(self, query: str, context: List[str]) -> str:\n",
    "        prompt = self.prompt_template(query, context)\n",
    "        output = self.llm.generate([prompt], self.sampling_params)\n",
    "        return output[0].outputs[0].text\n",
    "    \n",
    "    def answer_query(self, query: str, top_k: int = 5) -> str:\n",
    "        retrieved_contexts = self.process_query(query, top_k)\n",
    "        return self.generate_response(query, retrieved_contexts)\n",
    "    \n",
    "class LLaVAImageQAProcessor:\n",
    "    def __init__(self):\n",
    "        self.llm = llm\n",
    "        self.sampling_params = sampling_params\n",
    "\n",
    "    def get_prompt(self, question: str):\n",
    "        return f\"\"\"[INST] \n",
    "                    Explain me about this image precisely in bullet points. \n",
    "                    Your response shuold be in complete sentences.\n",
    "                    <image>\\n{question} [/INST]\"\"\"\n",
    "\n",
    "    def process_image(self, image_path, question):\n",
    "        prompt = self.get_prompt(question)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        inputs = {\n",
    "            \"prompt\": prompt,\n",
    "            \"multi_modal_data\": {\"image\": image},\n",
    "        }\n",
    "\n",
    "        outputs = self.llm.generate(inputs, sampling_params=self.sampling_params)\n",
    "        return outputs[0].outputs[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
