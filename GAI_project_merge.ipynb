{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP90RyKB2mM1l+LgV4Bc7QB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuMinHo/GAI_project/blob/main/GAI_project_merge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MImKlDkPjxOI"
      },
      "outputs": [],
      "source": [
        "!pip install gradio vllm transformers triton PyPDF2 Pillow sentence_transformers numpy typing faiss-gpu spacy pymupdf4llm fitz frontend tools semchunk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import faiss\n",
        "import numpy as np\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "import time\n",
        "import semchunk\n",
        "import pymupdf as fitz\n",
        "import pymupdf4llm\n",
        "from vllm import LLM, SamplingParams\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from PIL import Image\n",
        "import hashlib\n",
        "import logging\n",
        "import torch\n",
        "import gc"
      ],
      "metadata": {
        "id": "KxSgSptFlDw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전역 변수 초기화\n",
        "llm = LLM(model=\"llava-hf/llava-v1.6-mistral-7b-hf\", dtype='half', max_model_len=8192)\n",
        "sampling_params = SamplingParams(temperature=0.7, max_tokens=512)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\"\"\"\n",
        "PDF 파일 RAG를 위한 Pipeline class\n",
        "\"\"\"\n",
        "class RAGPipeline:\n",
        "    def __init__(self):\n",
        "        # lava-hf/llava-v1.6-mistral-7b-hf를 사용\n",
        "        self.llm = llm\n",
        "\n",
        "        # Sampling parameters 설정\n",
        "        self.sampling_params = sampling_params\n",
        "\n",
        "        # embedding\n",
        "        self.embedder = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "        self.chunker = semchunk.chunkerify('gpt-4', 200)\n",
        "        self.index = faiss.IndexFlatL2(self.embedder.get_sentence_embedding_dimension())\n",
        "        self.chunks = []\n",
        "        self.processed_files = {} # {file_hash: file_path}\n",
        "\n",
        "    def get_file_hash(self, file_path: str) -> str:\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            return hashlib.md5(f.read()).hexdigest()\n",
        "\n",
        "    def indexing_pdf(self, pdf_path: List[str]):\n",
        "        for pdf in pdf_path:\n",
        "            try:\n",
        "                file_hash = self.get_file_hash(pdf)\n",
        "                if file_hash in self.processed_files:\n",
        "                    logging.info(f\"{pdf} has already been processed before\")\n",
        "                    continue\n",
        "\n",
        "                self.processed_files[file_hash] = pdf\n",
        "                logging.info(f\"Processing new file: {pdf}\")\n",
        "\n",
        "                doc = fitz.open(pdf)\n",
        "                markdown_text = pymupdf4llm.to_markdown(doc)\n",
        "                doc.close()\n",
        "\n",
        "                chunks = self.chunker(markdown_text)\n",
        "                self.chunks.extend(chunks)\n",
        "                chunks_embeddings = self.embedder.encode(chunks)\n",
        "                self.index.add(chunks_embeddings)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error in indexing {pdf_path}: {e}\")\n",
        "\n",
        "        logging.info(f\"Processed {len(pdf_path)} files. Total unique files: {len(self.processed_files)}\")\n",
        "\n",
        "    def process_query(self, query: str, top_k: int = 5) -> List[str]:\n",
        "        query_embedding = self.embedder.encode([query])\n",
        "        distances, indices = self.index.search(query_embedding, top_k)\n",
        "        return [self.chunks[i] for i in indices[0]]\n",
        "\n",
        "    def prompt_template(self, query: str, context: List[str]) -> str:\n",
        "        system_message = \"\"\"You are an AI assistant tasked with answering questions based on provided context. Your role is to:\n",
        "                            1. Carefully analyze the given context\n",
        "                            2. Provide accurate and relevant information\n",
        "                            3. Synthesize a coherent response\n",
        "                            4. Maintain objectivity and clarity\n",
        "                            If the context doesn't contain sufficient information, state so clearly.\"\"\"\n",
        "\n",
        "        context_str = \"\\n\".join([f\"Context {i+1}: {ctx}\" for i, ctx in enumerate(context)])\n",
        "\n",
        "        prompt = f\"\"\"[INST] {system_message}\n",
        "\n",
        "            Relevant information:\n",
        "            {context_str}\n",
        "\n",
        "            User's Quetion: {query}\n",
        "\n",
        "            Instructions:\n",
        "            - Answer the query using only the information provided in the context.\n",
        "            - If the context doesn't contain enough information to fully answer the query, acknowledge this limitation in your response.\n",
        "            - Provide a concise yet comprehensive answer.\n",
        "            - Do not introduce information not present in the given context.\n",
        "            - Privide in complete sentences in English always.\n",
        "            - Check once again your response so that the user can be provided precise information.\n",
        "\n",
        "            Please provide your response below:\n",
        "            [/INST]\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def generate_response(self, query: str, context: List[str]) -> str:\n",
        "        prompt = self.prompt_template(query, context)\n",
        "        output = self.llm.generate([prompt], self.sampling_params)\n",
        "        return output[0].outputs[0].text\n",
        "\n",
        "    def answer_query(self, query: str, top_k: int = 5) -> str:\n",
        "        retrieved_contexts = self.process_query(query, top_k)\n",
        "        return self.generate_response(query, retrieved_contexts)\n",
        "\n",
        "class LLaVAImageQAProcessor:\n",
        "    def __init__(self):\n",
        "        self.llm = llm\n",
        "        self.sampling_params = sampling_params\n",
        "\n",
        "    def get_prompt(self, question: str):\n",
        "        return f\"\"\"[INST]\n",
        "                    Explain me about this image precisely in bullet points.\n",
        "                    Your response should be in complete sentences.\n",
        "                    [/INST]\"\"\"\n",
        "\n",
        "    def process_image(self, image: Image.Image, question: str) -> str:\n",
        "        prompt = self.get_prompt(question)\n",
        "        try:\n",
        "            inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"image\": image}}\n",
        "            outputs = self.llm.generate(inputs, self.sampling_params)\n",
        "            return outputs[0].outputs[0].text.strip() if outputs else \"Failed to generate response.\"\n",
        "        finally:\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iXiatGqn2lmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SessionManager:\n",
        "    def __init__(self):\n",
        "        self.sessions = {\n",
        "            \"Example\": {\n",
        "                \"history\": [],\n",
        "                \"mode\": \"General Chat\",\n",
        "                \"mode_locked\": False,\n",
        "                \"rag_pipeline\": RAGPipeline(),\n",
        "                \"img_processor\": LLaVAImageQAProcessor()\n",
        "            }\n",
        "        }\n",
        "        self.current_session = \"Example\"\n",
        "\n",
        "    def create_session(self, session_name: str) -> bool:\n",
        "        if session_name in self.sessions:\n",
        "            return False\n",
        "\n",
        "        self.sessions[session_name] = {\n",
        "            \"history\": [],\n",
        "            \"mode\": \"General Chat\",\n",
        "            \"mode_locked\": False,\n",
        "            \"rag_pipeline\": RAGPipeline(),\n",
        "            \"img_processor\": LLaVAImageQAProcessor()\n",
        "        }\n",
        "        self.current_session = session_name\n",
        "        return True\n",
        "\n",
        "    def delete_session(self, session_name: str) -> Tuple[str, dict]:\n",
        "        if len(self.sessions) <= 1:\n",
        "            return None, None\n",
        "\n",
        "        if session_name in self.sessions:\n",
        "            del self.sessions[session_name]\n",
        "            next_session = next(iter(self.sessions.keys()))\n",
        "            self.current_session = next_session\n",
        "            return next_session, self.sessions[next_session]\n",
        "        return None, None\n",
        "\n",
        "    def get_session(self, session_name: str) -> Optional[dict]:\n",
        "        return self.sessions.get(session_name)\n",
        "\n",
        "def create_ui():\n",
        "    session_manager = SessionManager()\n",
        "\n",
        "    custom_css = \"\"\"\n",
        "    .message-box {\n",
        "        display: flex;\n",
        "        align-items: center;\n",
        "        gap: 0.5rem;\n",
        "    }\n",
        "    .file-btn {\n",
        "        max-width: 40px;\n",
        "    }\n",
        "    .send-btn {\n",
        "        max-width: 40px;\n",
        "    }\n",
        "    .selected-file {\n",
        "        margin: 0.5rem 0;\n",
        "        padding: 0.3rem;\n",
        "        background: #f0f0f0;\n",
        "        border-radius: 4px;\n",
        "        font-size: 0.9em;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(css=custom_css) as demo:\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                new_session_btn = gr.Button(\"+ New Session\")\n",
        "                session_title_input = gr.Textbox(\n",
        "                    label=\"Session Title\",\n",
        "                    visible=False\n",
        "                )\n",
        "                with gr.Column(elem_classes=\"session-container\"):\n",
        "                    gr.Markdown(\"Sessions\")\n",
        "                    session_list = gr.Radio(\n",
        "                        choices=[\"Example\"],\n",
        "                        value=\"Example\",\n",
        "                        label=\"\"\n",
        "                    )\n",
        "                    delete_btn = gr.Button(\"🗑️ Delete Session\")\n",
        "\n",
        "            with gr.Column(scale=3):\n",
        "                current_title = gr.Markdown(\"## Example\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    chat_mode = gr.Radio(\n",
        "                        choices=[\"General Chat\", \"RAG Chat\"],\n",
        "                        value=\"General Chat\",\n",
        "                        label=\"\"\n",
        "                    )\n",
        "\n",
        "                chatbot = gr.Chatbot(\n",
        "                    height=400,\n",
        "                    render_markdown=True,\n",
        "                    show_copy_button=True,\n",
        "                    bubble_full_width=False\n",
        "                )\n",
        "\n",
        "                # 파일 업로드, 메시지 입력, 전송 버튼을 한 줄로\n",
        "                with gr.Row():\n",
        "                    # General Chat 모드용 이미지 업로드\n",
        "                    with gr.Column(scale=1, visible=True) as general_upload:\n",
        "                        file_upload_image = gr.UploadButton(\n",
        "                            \"📎\",\n",
        "                            file_types=[\".jpg\", \".jpeg\", \".png\"]\n",
        "                        )\n",
        "\n",
        "                    # RAG Chat 모드용 PDF 업로드\n",
        "                    with gr.Column(scale=1, visible=False) as rag_upload:\n",
        "                        file_upload_pdf = gr.File(\n",
        "                            label=\"PDF\",\n",
        "                            file_types=[\".pdf\"],\n",
        "                            file_count=\"multiple\"\n",
        "                        )\n",
        "\n",
        "                    # 메시지 입력창\n",
        "                    msg = gr.Textbox(\n",
        "                        show_label=False,\n",
        "                        placeholder=\"메시지를 입력하세요...\",\n",
        "                        container=False,\n",
        "                        scale=8\n",
        "                    )\n",
        "\n",
        "                    # 전송 버튼\n",
        "                    send_btn = gr.Button(\"↑\", scale=1)\n",
        "\n",
        "                with gr.Row():\n",
        "                    clear_btn = gr.Button(\"Clear Chat\")\n",
        "\n",
        "                with gr.Row(visible=True) as general_file_info:\n",
        "                    selected_image = gr.Textbox(\n",
        "                        label=\"Selected Image\",\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "                with gr.Row(visible=False) as rag_file_info:\n",
        "                    selected_pdf = gr.Textbox(\n",
        "                        label=\"Selected PDF\",\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "        def process_message(message, file_image, files_pdf, mode, history, session_name):\n",
        "            try:\n",
        "                session = session_manager.get_session(session_name)\n",
        "                if not session:\n",
        "                    return \"세션을 찾을 수 없습니다.\"\n",
        "\n",
        "                current_mode = session[\"mode\"]\n",
        "\n",
        "                if current_mode == \"General Chat\":\n",
        "                    if file_image and file_image.name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        with Image.open(file_image) as image:\n",
        "                            if image.mode != 'RGB':\n",
        "                                image = image.convert('RGB')\n",
        "                            question = message if message.strip() else \"이 이미지에 대해 설명해주세요.\"\n",
        "                            return session[\"img_processor\"].process_image(image, question)\n",
        "                    else:\n",
        "                        prompt = f\"[INST] {message} [/INST]\"\n",
        "                        output = llm.generate([prompt], sampling_params)\n",
        "                        return output[0].outputs[0].text.strip()\n",
        "\n",
        "                else:  # RAG Chat mode\n",
        "                    if files_pdf:\n",
        "                        pdf_paths = [f.name for f in files_pdf]\n",
        "                        session[\"rag_pipeline\"].indexing_pdf(pdf_paths)\n",
        "                        return f\"{len(pdf_paths)}개의 PDF가 성공적으로 처리되었습니다. 이제 문서에 대해 질문할 수 있습니다.\"\n",
        "\n",
        "                    return session[\"rag_pipeline\"].answer_query(message)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"메시지 처리 중 오류: {str(e)}\")\n",
        "                return f\"오류가 발생했습니다: {str(e)}\"\n",
        "\n",
        "        def chat_mode_change(mode, session_name):\n",
        "            session = session_manager.get_session(session_name)\n",
        "            if not session:\n",
        "                return [gr.update()] * 5\n",
        "\n",
        "            if session[\"mode_locked\"]:\n",
        "                gr.Warning(\"대화가 시작된 후에는 모드를 변경할 수 없습니다. 새 세션을 만들어주세요.\")\n",
        "                current_mode = session[\"mode\"]\n",
        "                is_general = current_mode == \"General Chat\"\n",
        "                return [\n",
        "                    gr.update(value=current_mode),\n",
        "                    gr.update(visible=is_general),\n",
        "                    gr.update(visible=not is_general),\n",
        "                    gr.update(visible=is_general),\n",
        "                    gr.update(visible=not is_general)\n",
        "                ]\n",
        "\n",
        "            session[\"mode\"] = mode\n",
        "            is_general = mode == \"General Chat\"\n",
        "            return [\n",
        "                gr.update(value=mode),\n",
        "                gr.update(visible=is_general),\n",
        "                gr.update(visible=not is_general),\n",
        "                gr.update(visible=is_general),\n",
        "                gr.update(visible=not is_general)\n",
        "            ]\n",
        "\n",
        "        def send_message(message, file_image, files_pdf, session_name, mode, history):\n",
        "            if not message.strip() and not (file_image or files_pdf):\n",
        "                return history, \"\", None, None, \"\", \"\"\n",
        "\n",
        "            try:\n",
        "                session = session_manager.get_session(session_name)\n",
        "                if not session:\n",
        "                    return history, \"\", None, None, \"\", \"\"\n",
        "\n",
        "                if not session[\"mode_locked\"] and (message.strip() or file_image or files_pdf):\n",
        "                    session[\"mode_locked\"] = True\n",
        "                    session[\"mode\"] = mode\n",
        "\n",
        "                current_mode = session[\"mode\"]\n",
        "                response = process_message(message, file_image if current_mode == \"General Chat\" else None,\n",
        "                                        files_pdf if current_mode == \"RAG Chat\" else None,\n",
        "                                        current_mode, history, session_name)\n",
        "\n",
        "                if current_mode == \"General Chat\" and file_image:\n",
        "                    history.append(((file_image.name, file_image), message if message.strip() else None))\n",
        "                elif current_mode == \"RAG Chat\" and files_pdf:\n",
        "                    pdf_names = [f.name for f in files_pdf]\n",
        "                    history.append((f\"Uploaded PDFs: {', '.join(pdf_names)}\", None))\n",
        "                else:\n",
        "                    history.append((None, message))\n",
        "\n",
        "                history.append((None, response))\n",
        "                session[\"history\"] = history\n",
        "\n",
        "                return history, \"\", None, None, \"\", \"\"\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"메시지 전송 중 오류: {str(e)}\")\n",
        "                return history, \"\", None, None, \"\", \"\"\n",
        "        def add_session(title):\n",
        "            if not title:\n",
        "                return gr.update(visible=False), gr.update(choices=list(session_manager.sessions.keys()))\n",
        "\n",
        "            if session_manager.create_session(title):\n",
        "                return gr.update(visible=False), gr.update(choices=list(session_manager.sessions.keys()), value=title)\n",
        "            else:\n",
        "                gr.Warning(\"이미 존재하는 세션 이름입니다.\")\n",
        "                return gr.update(visible=False), gr.update(choices=list(session_manager.sessions.keys()))\n",
        "\n",
        "        def switch_session(session_name):\n",
        "            session = session_manager.get_session(session_name)\n",
        "            if session:\n",
        "                session_manager.current_session = session_name\n",
        "                return (\n",
        "                    f\"## {session_name}\",\n",
        "                    session[\"history\"],\n",
        "                    session[\"mode\"]\n",
        "                )\n",
        "            return current_title, [], chat_mode.value\n",
        "\n",
        "        def delete_session(session_name):\n",
        "            next_session, session_data = session_manager.delete_session(session_name)\n",
        "            if next_session is None:\n",
        "                gr.Warning(\"마지막 세션은 삭제할 수 없습니다\")\n",
        "                return (\n",
        "                    gr.update(choices=list(session_manager.sessions.keys()), value=session_name),\n",
        "                    current_title,\n",
        "                    chatbot,\n",
        "                    chat_mode\n",
        "                )\n",
        "\n",
        "            return (\n",
        "                gr.update(choices=list(session_manager.sessions.keys()), value=next_session),\n",
        "                f\"## {next_session}\",\n",
        "                session_data[\"history\"],\n",
        "                session_data[\"mode\"]\n",
        "            )\n",
        "\n",
        "        def clear_chat():\n",
        "            return [], \"\", \"\"\n",
        "\n",
        "        # 이벤트 바인딩\n",
        "        new_session_btn.click(\n",
        "            lambda: gr.update(visible=True),\n",
        "            outputs=session_title_input\n",
        "        )\n",
        "\n",
        "        session_title_input.submit(\n",
        "            add_session,\n",
        "            inputs=[session_title_input],\n",
        "            outputs=[session_title_input, session_list]\n",
        "        )\n",
        "\n",
        "        session_list.change(\n",
        "            switch_session,\n",
        "            inputs=[session_list],\n",
        "            outputs=[current_title, chatbot, chat_mode]\n",
        "        )\n",
        "\n",
        "        delete_btn.click(\n",
        "            delete_session,\n",
        "            inputs=[session_list],\n",
        "            outputs=[session_list, current_title, chatbot, chat_mode]\n",
        "        )\n",
        "\n",
        "        send_btn.click(\n",
        "            send_message,\n",
        "            inputs=[msg, file_upload_image, file_upload_pdf, session_list, chat_mode, chatbot],\n",
        "            outputs=[chatbot, msg, file_upload_image, file_upload_pdf, selected_image, selected_pdf]\n",
        "        )\n",
        "\n",
        "        msg.submit(\n",
        "            send_message,\n",
        "            inputs=[msg, file_upload_image, file_upload_pdf, session_list, chat_mode, chatbot],\n",
        "            outputs=[chatbot, msg, file_upload_image, file_upload_pdf, selected_image, selected_pdf]\n",
        "        )\n",
        "\n",
        "        chat_mode.change(\n",
        "            chat_mode_change,\n",
        "            inputs=[chat_mode, session_list],\n",
        "            outputs=[chat_mode, general_upload, rag_upload, general_file_info, rag_file_info]\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            clear_chat,\n",
        "            outputs=[chatbot, selected_image, selected_pdf]\n",
        "        )\n",
        "\n",
        "        return demo\n",
        "\n",
        "# GUI 실행\n",
        "demo = create_ui()\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "NQ_Ze9vFlKia"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}