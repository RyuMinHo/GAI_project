{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPe6RRxD/YooIsmC9/kqRqQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyuMinHo/GAI_project/blob/main/GUI_GAI\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "import faiss\n",
        "import numpy as np\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PyPDF2 import PdfReader\n",
        "import semchunk\n",
        "from PIL import Image\n",
        "import pymupdf4llm\n",
        "import pymupdf as fitz\n",
        "import os\n",
        "from vllm import LLM, SamplingParams\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "# 모델 초기화\n",
        "def init_models():\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"CUDA device available: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "        print(\"Loading LLaVA model...\")\n",
        "        llm = LLM(\n",
        "            model=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
        "            dtype='half',\n",
        "            max_model_len=8192,\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "            gpu_memory_utilization=0.9\n",
        "        )\n",
        "\n",
        "        print(\"Loading SentenceTransformer...\")\n",
        "        embedder = SentenceTransformer(\n",
        "            'paraphrase-multilingual-mpnet-base-v2',\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "\n",
        "        print(\"All models initialized successfully!\")\n",
        "        return llm, embedder\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model initialization: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "GLOBAL_LLM, GLOBAL_EMBEDDER = init_models()\n",
        "if GLOBAL_LLM is None or GLOBAL_EMBEDDER is None:\n",
        "    raise RuntimeError(\"Failed to initialize one or more required models\")\n",
        "\n",
        "# FAISS 인덱스 및 청크를 관리하는 클래스\n",
        "class SessionIndexManager:\n",
        "    def __init__(self, embedder):\n",
        "        self.embedder = embedder\n",
        "        self.session_indices = {}\n",
        "        self.session_chunks = {}\n",
        "\n",
        "    def create_session_index(self, session_id):\n",
        "        dimension = self.embedder.get_sentence_embedding_dimension()\n",
        "        index = faiss.IndexFlatL2(dimension)\n",
        "        self.session_indices[session_id] = index\n",
        "        self.session_chunks[session_id] = []\n",
        "\n",
        "    def add_document(self, session_id, chunks):\n",
        "        if session_id not in self.session_indices:\n",
        "            self.create_session_index(session_id)\n",
        "\n",
        "        embeddings = self.embedder.encode(chunks)\n",
        "        self.session_indices[session_id].add(np.array(embeddings))\n",
        "        self.session_chunks[session_id].extend(chunks)\n",
        "\n",
        "    def search(self, session_id, query, k=5):\n",
        "        if session_id not in self.session_indices:\n",
        "            return []\n",
        "        query_embedding = self.embedder.encode([query])\n",
        "        D, I = self.session_indices[session_id].search(\n",
        "            np.array(query_embedding), k\n",
        "        )\n",
        "        return [self.session_chunks[session_id][i] for i in I[0]]\n",
        "\n",
        "    def delete_session(self, session_id):\n",
        "        if session_id in self.session_indices:\n",
        "            del self.session_indices[session_id]\n",
        "        if session_id in self.session_chunks:\n",
        "            del self.session_chunks[session_id]\n",
        "\n",
        "index_manager = SessionIndexManager(GLOBAL_EMBEDDER)\n",
        "\n",
        "# PDF 파일 처리\n",
        "def process_pdf_to_chunks(file):\n",
        "    try:\n",
        "        pdf_reader = PdfReader(file)\n",
        "        text_chunks = []\n",
        "        for page in pdf_reader.pages:\n",
        "            text = page.extract_text().strip()\n",
        "            if text:\n",
        "                text_chunks.append(text)\n",
        "        print(f\"Successfully processed PDF with {len(text_chunks)} pages\")\n",
        "        return text_chunks\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing PDF: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# 메시지 처리 함수\n",
        "def process_message(message, file, mode, history, session_name):\n",
        "    if mode == \"General Chat\":\n",
        "        if not file:\n",
        "            try:\n",
        "                prompt = message\n",
        "                response = GLOBAL_LLM.generate(\n",
        "                    prompts=[prompt],\n",
        "                    sampling_params=SamplingParams(\n",
        "                        temperature=0.15, max_tokens=512, top_p=0.85, repetition_penalty=1.18\n",
        "                    )\n",
        "                )\n",
        "                return response[0].outputs[0].text.strip()\n",
        "            except Exception as e:\n",
        "                return f\"Error processing text: {str(e)}\"\n",
        "\n",
        "        else:\n",
        "            try:\n",
        "                with Image.open(file) as image:\n",
        "                    if image.mode != 'RGB':\n",
        "                        image = image.convert('RGB')\n",
        "\n",
        "                    prompt = f\"\"\"[INST] <image>\n",
        "                    Please describe this image in detail:\n",
        "\n",
        "                    Question: {message}\n",
        "\n",
        "                    [/INST]\"\"\"\n",
        "                    inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"image\": image}}\n",
        "                    outputs = GLOBAL_LLM.generate(inputs, sampling_params=SamplingParams())\n",
        "                    return outputs[0].outputs[0].text.strip()\n",
        "            except Exception as e:\n",
        "                return f\"Error processing image: {str(e)}\"\n",
        "\n",
        "    else:  # RAG Chat\n",
        "        try:\n",
        "            if file and file.name.endswith('.pdf'):\n",
        "                chunks = process_pdf_to_chunks(file)\n",
        "                if not chunks:\n",
        "                    return \"Failed to process PDF file.\"\n",
        "                index_manager.add_document(session_name, chunks)\n",
        "                return f\"PDF processed successfully. Extracted {len(chunks)} pages.\"\n",
        "\n",
        "            if session_name not in index_manager.session_indices:\n",
        "                return \"Please upload a PDF document first.\"\n",
        "\n",
        "            relevant_chunks = index_manager.search(session_name, message)\n",
        "            if not relevant_chunks:\n",
        "                return \"No relevant information found.\"\n",
        "\n",
        "            context = \"\\n\".join(relevant_chunks)\n",
        "            prompt = f\"\"\"Context:\\n{context}\\n\\nQuestion: {message}\"\"\"\n",
        "            response = GLOBAL_LLM.generate(\n",
        "                prompts=[prompt],\n",
        "                sampling_params=SamplingParams(\n",
        "                    temperature=0.15, max_tokens=512, top_p=0.85, repetition_penalty=1.18\n",
        "                )\n",
        "            )\n",
        "            return response[0].outputs[0].text.strip()\n",
        "        except Exception as e:\n",
        "            return f\"Error in RAG processing: {str(e)}\"\n",
        "\n",
        "# UI 생성\n",
        "def create_ui():\n",
        "    with gr.Blocks() as demo:\n",
        "        with gr.Row():\n",
        "            chat_mode = gr.Radio(choices=[\"General Chat\", \"RAG Chat\"], value=\"General Chat\")\n",
        "            chatbot = gr.Chatbot()\n",
        "\n",
        "            file_upload = gr.UploadButton(file_types=[\".jpg\", \".jpeg\", \".png\", \".pdf\"])\n",
        "            msg = gr.Textbox()\n",
        "            send_btn = gr.Button(\"Send\")\n",
        "\n",
        "        send_btn.click(\n",
        "            lambda message, file, mode: process_message(message, file, mode, [], \"session1\"),\n",
        "            inputs=[msg, file_upload, chat_mode],\n",
        "            outputs=chatbot\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_ui()\n",
        "    demo.launch()\n"
      ],
      "metadata": {
        "id": "J8CieSejtKCa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}